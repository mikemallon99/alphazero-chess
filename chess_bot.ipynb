{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtrhJa4AMz3G",
        "outputId": "62d16183-ea74-4a7d-fb04-d29fa6c2349b"
      },
      "outputs": [],
      "source": [
        "!pip install torch numpy chess graphviz==0.20.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWdcxUB3S4gn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "import chess\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4d4iFfezpEr",
        "outputId": "f125d661-2726-4b49-8a4c-4bf9fd87f374"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i0EgWUH2jntM"
      },
      "source": [
        "Need to add some kind of arena to this model. Currently the model is improving its ability to predict the mcts process, but there is not a guarantee that the mcts process will be very good. Its a possibility that im not doing enough training iterations, which would mean that im consistently getting bad data. Its possible that theres a bug in my MCTS which would make my implementation give bad results, and then the model would continue to train on the bad results. So its possible that I just need better data (run for more iterations) to have better games from my mcts.  \n",
        "\n",
        "What is the point in having the old network and the new network compete against itself? Why not just have the network constantly do self play to generate data and then train on that data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjdzs9wnMhbT"
      },
      "outputs": [],
      "source": [
        "# Search tree\n",
        "# tree = [((n, w, q, p), [((n2, w2, q2, p2) [...])]), ... ((n_n, w_n, q_n, p_n), [...])]\n",
        "# whole tree is stored in numpy array? how to do this\n",
        "# t(s,a) = (n(s,a), w(s,a), q(s,a), p(s,a))\n",
        "# how to encode (s,a) -> s'? this could just be the game rules\n",
        "# Need a way to discard the rest of the tree when its not needed anymore\n",
        "# is s an encoding of the board state?\n",
        "# not sure if we can make a numpy array of size s x a, because s is massive\n",
        "# numpy array doesnt need to be indexed like a function\n",
        "# p(s,a) is just a 2d numpy array of probabilities\n",
        "# n(s,a) is the number of times action a has been taken in state s\n",
        "# we know that each node in the tree will have the same number of actions, which means we can know the number of nodes in the tree based on the size of the numpy array\n",
        "\n",
        "# Chess numbers\n",
        "T = 8\n",
        "N = 8\n",
        "M = 6\n",
        "L = 7\n",
        "\n",
        "past_boards = {}\n",
        "\n",
        "def itb(num: int, length: int):\n",
        "    \"\"\"\n",
        "    Converts integer to bit array\n",
        "    Someone fix this please :D - it's horrible\n",
        "    :param num: number to convert to bits\n",
        "    :param length: length of bits to convert to\n",
        "    :return: bit array\n",
        "    \"\"\"\n",
        "    num = int(num)\n",
        "    if length == 1:\n",
        "        return [int(i) for i in '{0:01b}'.format(num)]\n",
        "    if length == 2:\n",
        "        return [int(i) for i in '{0:02b}'.format(num)]\n",
        "    if length == 3:\n",
        "        return [int(i) for i in '{0:03b}'.format(num)]\n",
        "    if length == 4:\n",
        "        return [int(i) for i in '{0:04b}'.format(num)]\n",
        "    if length == 5:\n",
        "        return [int(i) for i in '{0:05b}'.format(num)]\n",
        "    if length == 8:\n",
        "        return [int(i) for i in '{0:08b}'.format(num)]\n",
        "    if length == 11:\n",
        "        return [int(i) for i in '{0:011b}'.format(num)]\n",
        "    raise TypeError(\"Length not supported:\", length)\n",
        "\n",
        "REPETITION_IDX_START = 12\n",
        "REPETITION_IDX_END = REPETITION_IDX_START + 2\n",
        "COLOR_IDX = REPETITION_IDX_END\n",
        "MOVE_COUNT_IDX = COLOR_IDX + 1\n",
        "P1_CASTLING_IDX_KINGSIDE = MOVE_COUNT_IDX + 1\n",
        "P1_CASTLING_IDX_QUEENSIDE = P1_CASTLING_IDX_KINGSIDE + 1\n",
        "P2_CASTLING_IDX_KINGSIDE = P1_CASTLING_IDX_QUEENSIDE + 1\n",
        "P2_CASTLING_IDX_QUEENSIDE = P2_CASTLING_IDX_KINGSIDE + 1\n",
        "NO_PROGRESS_IDX = P2_CASTLING_IDX_QUEENSIDE + 1\n",
        "\n",
        "def encode_board(board: chess.Board) -> np.ndarray:\n",
        "    b = np.zeros((N, N, 6+6+2+1+1+2+2+1))\n",
        "\n",
        "    for x in range(0,8):\n",
        "        for y in range(0,8):\n",
        "            piece = board.piece_at(chess.square(x,y))\n",
        "            if piece is None:\n",
        "                continue\n",
        "            piece_type = int(piece.piece_type) - 1\n",
        "            piece_color = piece.color\n",
        "            if piece_color == chess.BLACK:\n",
        "                piece_type += 6\n",
        "            # Piece encoding\n",
        "            b[x][y][piece_type] = 1\n",
        "\n",
        "    # Repetition encoding\n",
        "    # TODO : incorporate repetition encoding\n",
        "    b[:, :, REPETITION_IDX_START:REPETITION_IDX_END] = itb(0, 2)\n",
        "\n",
        "    # OTHER ENCODINGS\n",
        "    b[:, :, COLOR_IDX] = 1 if board.turn == chess.BLACK else 0\n",
        "    b[:, :, MOVE_COUNT_IDX] = board.fullmove_number \n",
        "    b[:, :, P1_CASTLING_IDX_KINGSIDE] = 1 if board.has_kingside_castling_rights(chess.WHITE) else 0\n",
        "    b[:, :, P1_CASTLING_IDX_QUEENSIDE] = 1 if board.has_queenside_castling_rights(chess.WHITE) else 0\n",
        "    b[:, :, P2_CASTLING_IDX_KINGSIDE] = 1 if board.has_kingside_castling_rights(chess.BLACK) else 0\n",
        "    b[:, :, P2_CASTLING_IDX_QUEENSIDE] = 1 if board.has_queenside_castling_rights(chess.BLACK) else 0\n",
        "    b[:, :, NO_PROGRESS_IDX] = board.halfmove_clock / 2\n",
        "    \n",
        "    return b.astype(np.float32)\n",
        "\n",
        "# Moves = 7 * 8 queen moves + 8 knight moves + 9 underpromotions \n",
        "\n",
        "def encode_move(move):\n",
        "    from_square = int(move.from_square)\n",
        "    from_square_x = from_square % 8\n",
        "    from_square_y = from_square // 8\n",
        "\n",
        "    to_square = int(move.to_square)\n",
        "    to_square_x = to_square % 8\n",
        "    to_square_y = to_square // 8\n",
        "\n",
        "    # Moves are stored like this\n",
        "    # 0-6 north, 7-13 south, 14-20 east, 21-27 west, 28-34 northeast, 35-41 northwest, 42-48 southeast, 49-55 southwest\n",
        "    dx = to_square_x - from_square_x\n",
        "    dy = to_square_y - from_square_y\n",
        "\n",
        "    # Queen moves\n",
        "    if dy > 0 and dx == 0:\n",
        "        idx = dy\n",
        "    elif dy < 0 and dx == 0:\n",
        "        idx = 7 + abs(dy)\n",
        "    elif dy == 0 and dx > 0:\n",
        "        idx = 14 + dx\n",
        "    elif dy == 0 and dx < 0:\n",
        "        idx = 21 + abs(dx)\n",
        "    elif dy > 0 and dx > 0 and abs(dy) == abs(dx):\n",
        "        idx = 28 + dx\n",
        "    elif dy > 0 and dx < 0 and abs(dy) == abs(dx):\n",
        "        idx = 35 + abs(dx)\n",
        "    elif dy < 0 and dx > 0 and abs(dy) == abs(dx):\n",
        "        idx = 42 + dx\n",
        "    elif dy < 0 and dx < 0 and abs(dy) == abs(dx):\n",
        "        idx = 49 + abs(dx)\n",
        "    # Knight moves\n",
        "    elif dx == 1 and dy == 2:\n",
        "        idx = 56\n",
        "    elif dx == 2 and dy == 1:\n",
        "        idx = 57\n",
        "    elif dx == -1 and dy == 2:\n",
        "        idx = 58\n",
        "    elif dx == -2 and dy == 1:\n",
        "        idx = 59\n",
        "    elif dx == 1 and dy == -2:\n",
        "        idx = 60\n",
        "    elif dx == 2 and dy == -1:\n",
        "        idx = 61\n",
        "    elif dx == -1 and dy == -2:\n",
        "        idx = 62\n",
        "    elif dx == -2 and dy == -1:\n",
        "        idx = 63\n",
        "    # Underpromotions\n",
        "    # Cases with no capture\n",
        "    elif abs(dx) == 0 and abs(dy) == 1 and move.promotion != chess.QUEEN:\n",
        "        idx = 64 + move.promotion - 1\n",
        "    # Capture northwest\n",
        "    elif dx == -1 and abs(dy) == 1 and move.promotion != chess.QUEEN:\n",
        "        idx = 67 + move.promotion - 1\n",
        "    # Capture northeast\n",
        "    elif dx == 1 and abs(dy) == 1 and move.promotion != chess.QUEEN:\n",
        "        idx = 70 + move.promotion - 1\n",
        "    \n",
        "    return (from_square_x * 8 * 8) + (from_square_y * 8) + idx\n",
        "\n",
        "\n",
        "# Monte Carlo Tree Search\n",
        "@torch.no_grad()\n",
        "def mcts_step(tree, net, board_in, c=1.0, temp=1.0, iterations=1000, epsilon=0.25):\n",
        "    if 'children' in tree.keys():\n",
        "        # Add dirichlet noise to root node to add variety\n",
        "        priors_list = []\n",
        "        for _, child_dict in tree['children'].items():\n",
        "            priors_list.append(child_dict['data'][3])\n",
        "        num_actions = len(priors_list)\n",
        "        dir = np.random.default_rng().dirichlet([0.3] * num_actions)\n",
        "\n",
        "        for i, (_, child_dict) in enumerate(tree['children'].items()):\n",
        "            child_dict['data'][3] = (1 - epsilon) * child_dict['data'][3] + epsilon * dir[i]\n",
        "\n",
        "    for _ in range(0, iterations):\n",
        "        board = board_in.copy()\n",
        "\n",
        "        # Selection step\n",
        "        traversed_nodes = []\n",
        "        cur_subtree = tree\n",
        "        while 'children' in cur_subtree:\n",
        "\n",
        "            # Get sum(N) for all children\n",
        "            n_sqrt_sum = math.sqrt(sum(move_data['data'][0] for _, move_data in cur_subtree['children'].items()))\n",
        "            move_max = float('-inf')\n",
        "            for move, move_data in cur_subtree['children'].items():\n",
        "                # Calculate U value\n",
        "                p = move_data['data'][3]\n",
        "                u = c * p * (n_sqrt_sum / (1 + move_data['data'][0]))\n",
        "\n",
        "                # Calculate Q value\n",
        "                q = move_data['data'][2]\n",
        "                \n",
        "                if (q + u) > move_max:\n",
        "                    move_max = q + u\n",
        "                    best_move = move\n",
        "\n",
        "            if best_move not in cur_subtree['children'].keys():\n",
        "                print(cur_subtree)\n",
        "\n",
        "            # Append node data to list for backpropagation\n",
        "            traversed_nodes.append(cur_subtree['children'][best_move])\n",
        "\n",
        "            # Move to next node\n",
        "            cur_subtree = cur_subtree['children'][best_move]\n",
        "            board.push_uci(best_move)\n",
        "        \n",
        "        # Expansion step\n",
        "        # Add leaf node to queue for neural network evaluation\n",
        "        \n",
        "        # Handle reaching a terminal state\n",
        "        if board.legal_moves.count() == 0:\n",
        "            outcome = board.outcome()\n",
        "            # either a draw or the current player lost\n",
        "            if outcome.winner is None:\n",
        "                v = 0\n",
        "            else:\n",
        "                v = -1\n",
        "            # print(f\"reached terminal state {v}\")\n",
        "        else:\n",
        "            board_encoded = encode_board(board)\n",
        "\n",
        "            # Send to neural network\n",
        "            p, v = net(torch.from_numpy(board_encoded).to('cuda'))\n",
        "            p = p.cpu()\n",
        "            v = v.cpu()\n",
        "\n",
        "            # Collect all legal moves and their predicted values\n",
        "            legal_move_list = []\n",
        "            for move in board.legal_moves:\n",
        "                a = encode_move(move)\n",
        "                legal_move_list.append((p[0, a].item(), move))\n",
        "\n",
        "            # Normalize the p values. add constant to prevent divide by zero error\n",
        "            p_sum = sum(x[0] for x in legal_move_list)\n",
        "            if p_sum == 0:\n",
        "                legal_move_list = [(1/len(legal_move_list), x[1]) for x in legal_move_list]\n",
        "            else:\n",
        "                legal_move_list = [(x[0]/p_sum, x[1]) for x in legal_move_list]\n",
        "          \n",
        "            # Populate edges with predicted values if not terminal state\n",
        "            cur_subtree['children'] = {}\n",
        "            for p_val, move in legal_move_list:\n",
        "                if not (0 <= p_val <= 1):\n",
        "                    raise Exception(f\"PVAL ERROR: {p_val}\")\n",
        "                    print(p)\n",
        "                cur_subtree['children'][move.uci()] = {'data': np.array((0,0,0,p_val)), 'color': board.turn}\n",
        "\n",
        "            if not legal_move_list:\n",
        "                print(f\"SPOTTED PROBLEM: {board.legal_moves}\")\n",
        "\n",
        "        # Backup step\n",
        "        for node in traversed_nodes:\n",
        "            # N is the amount of times this edge has been traversed\n",
        "            node['data'][0] += 1\n",
        "            # W is the sum of all the values of this trajectory\n",
        "            if node['color'] == board.turn:\n",
        "                node['data'][1] += v\n",
        "            else:\n",
        "                node['data'][1] -= v\n",
        "            # Q is the average value of all the nodes beneath this edge\n",
        "            node['data'][2] = node['data'][1] / node['data'][0]\n",
        "\n",
        "    # Play move and discard rest of tree\n",
        "    encoded_policy = np.zeros((8 * 8 * 73))\n",
        "    if temp == 0:\n",
        "        # Pick max move\n",
        "        move_max = 0\n",
        "        selected_move = None\n",
        "        for move, move_data in tree['children'].items():\n",
        "            # Check that these probabilities are okay after move 30\n",
        "            policy = move_data['data'][0]\n",
        "\n",
        "            # Update max move\n",
        "            if policy > move_max:\n",
        "                move_max = policy\n",
        "                selected_move = move\n",
        "          \n",
        "        # Add as training datapoint\n",
        "        a = encode_move(chess.Move.from_uci(selected_move))\n",
        "        encoded_policy[a] = 1\n",
        "    else:\n",
        "        n_sqrt_sum_temp = sum(move_data['data'][0]**(1/temp) for _, move_data in tree['children'].items())\n",
        "        random_sample = random.random()\n",
        "        p = 0\n",
        "        selected_move = None\n",
        "        for move, move_data in tree['children'].items():\n",
        "            # Check that these probabilities are okay after move 30\n",
        "            policy = move_data['data'][0]**(1/temp) / n_sqrt_sum_temp\n",
        "            # Add as training datapoint\n",
        "            a = encode_move(chess.Move.from_uci(move))\n",
        "            encoded_policy[a] = policy\n",
        "\n",
        "            # See if this will be the selected move\n",
        "            p += policy\n",
        "            if selected_move is None and random_sample < p:\n",
        "                selected_move = move\n",
        "\n",
        "\n",
        "    # Create an encoding of the policy to return as a training datapoint\n",
        "    # Maybe just add together the values from above?\n",
        "\n",
        "    # Return a, new tree, and encoding of the policy\n",
        "    return selected_move, tree['children'][selected_move], encoded_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwqP5wJCfmZa"
      },
      "outputs": [],
      "source": [
        "import graphviz \n",
        "\n",
        "def draw_mcts(tree):\n",
        "    dot = graphviz.Digraph(format='png', graph_attr={'rankdir': 'LR'})\n",
        "    # dot.attr(size='6,6')\n",
        "    \n",
        "    dot.node(name = str(0), label = \"{ root | n: %d, w: %.4f, q: %.4f, p: %.4f }\" % (tree['data'][0], tree['data'][1], tree['data'][2], tree['data'][3]), shape='record')\n",
        "    id_counter = 0\n",
        "    queue = [(tree, 0)]\n",
        "    while queue:\n",
        "        subtree, parent_id = queue.pop(0)\n",
        "        if 'children' not in subtree.keys():\n",
        "            continue\n",
        "        for move_uci, values in subtree['children'].items():\n",
        "            id_counter += 1\n",
        "            dot.node(name = str(id_counter), label = \"{ %d | n: %d, w: %.4f, q: %.4f, p: %.4f }\" % (id_counter, values['data'][0], values['data'][1], values['data'][2], values['data'][3]), shape='record')\n",
        "            dot.edge(str(parent_id), str(id_counter))\n",
        "            queue.append((values, id_counter))\n",
        "    \n",
        "    # flat = dot.unflatten()\n",
        "\n",
        "    return dot"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ljNfDwCUZdXf"
      },
      "source": [
        "board = chess.Board()\n",
        "tree = {'data': np.array((0,0,0,1))}\n",
        "net.eval()\n",
        "mcts_step(tree, net, board)\n",
        "draw_mcts(tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAhQbt6NNH1j"
      },
      "outputs": [],
      "source": [
        "class TestNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TestNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(8*8*21, 1024)\n",
        "        self.layer2 = nn.Linear(1024, 1024)\n",
        "        self.layer3 = nn.Linear(1024, 1024)\n",
        "\n",
        "        # p\n",
        "        self.fc1 = nn.Linear(1024, 8*8*73)\n",
        "\n",
        "        # v\n",
        "        self.fc2 = nn.Linear(1024, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 8*8*21)\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = F.relu(self.layer3(x))\n",
        "        \n",
        "        p = F.softmax(self.fc1(x), dim=1)\n",
        "        v = torch.tanh(self.fc2(x))\n",
        "        return p, v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LFzjbhLn_Ki"
      },
      "outputs": [],
      "source": [
        "# Simple model of the data needed to make a training point\n",
        "class TrainingPoint:\n",
        "    def __init__(self, s: chess.Board, pi: np.ndarray, z=None) -> None:\n",
        "        self.s = s.copy()\n",
        "        self.pi = pi\n",
        "        self.z = z\n",
        "\n",
        "    def get_tensor(self): # Returns X, (pi, z)\n",
        "      # X: (8, 8, 21)\n",
        "      # pi: (8, 8, 73); z: 1\n",
        "      return torch.from_numpy(encode_board(self.s).astype(np.float32)), (torch.from_numpy(self.pi.astype(np.float32)), torch.tensor((self.z)))\n",
        "\n",
        "    def set_z_value(self, outcome: chess.Outcome):\n",
        "      if outcome.winner is None:\n",
        "          self.z = 0\n",
        "      elif outcome.winner == self.s.turn:\n",
        "          self.z = 1\n",
        "      else:\n",
        "          self.z = -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xB_8rn22NPzR"
      },
      "outputs": [],
      "source": [
        "def self_play(net, tree=None, iters=32):\n",
        "    # Self play and collect datapoints\n",
        "    datapoints = []\n",
        "    board = chess.Board()\n",
        "    if tree:\n",
        "        subtree = tree\n",
        "    else:\n",
        "        subtree = {'data': np.array((0,0,0,1))}\n",
        "    while board.outcome() is None:\n",
        "        # Change exploration depending on the move count\n",
        "        if board.fullmove_number < 30:\n",
        "            temp = 1\n",
        "        else:\n",
        "            temp = 0\n",
        "        move, subtree, pi = mcts_step(subtree, net, board, temp=temp, iterations=iters)\n",
        "        train_data = TrainingPoint(board.copy(), pi)\n",
        "        datapoints.append(train_data)\n",
        "        board.push_uci(move)\n",
        "\n",
        "    # Set the game outcome on all the datapoints\n",
        "    for datapoint in datapoints:\n",
        "        datapoint.set_z_value(board.outcome())\n",
        "\n",
        "    return datapoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUstNTHNSAcp"
      },
      "outputs": [],
      "source": [
        "def generate_train_data(net, episodes=100, iters=32):\n",
        "    # Feed in the same tree over and over again to introduce randomness\n",
        "    train_data = []\n",
        "    tree = {'data': np.array((0,0,0,1))}\n",
        "    for i in range(0, episodes):\n",
        "        train_data += self_play(net, tree=tree, iters=iters)\n",
        "        print(f\"Episode {i} complete\")\n",
        "    return train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyTmvr-3s7By"
      },
      "outputs": [],
      "source": [
        "# Loss function defined in the paper\n",
        "def loss(p, v, pi, z):\n",
        "    return (torch.sum((z - v.view(-1)).square()) - torch.sum(pi * torch.log(p + 1e-8))) / p.size()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8tGiNuV156z"
      },
      "outputs": [],
      "source": [
        "class ChessNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ChessNet, self).__init__()\n",
        "        self.num_channels = 512\n",
        "        self.dropout = 0.3\n",
        "        num_channels = self.num_channels\n",
        "        self.conv1 = nn.Conv2d(21, num_channels, 3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels, 3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(num_channels, num_channels, 3, stride=1)\n",
        "        self.conv4 = nn.Conv2d(num_channels, num_channels, 3, stride=1)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
        "        self.bn3 = nn.BatchNorm2d(num_channels)\n",
        "        self.bn4 = nn.BatchNorm2d(num_channels)\n",
        "\n",
        "        self.fc1 = nn.Linear(num_channels*(8-4)*(8-4), 1024)\n",
        "        self.fc_bn1 = nn.BatchNorm1d(1024)\n",
        "\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc_bn2 = nn.BatchNorm1d(512)\n",
        "\n",
        "        self.fc3 = nn.Linear(512, 8*8*73)\n",
        "\n",
        "        self.fc4 = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, s):\n",
        "        #                                                           s: batch_size x board_x x board_y\n",
        "        s = s.view(-1, 21, 8, 8)                # batch_size x 1 x board_x x board_y\n",
        "        s = F.relu(self.bn1(self.conv1(s)))                          # batch_size x num_channels x board_x x board_y\n",
        "        s = F.relu(self.bn2(self.conv2(s)))                          # batch_size x num_channels x board_x x board_y\n",
        "        s = F.relu(self.bn3(self.conv3(s)))                          # batch_size x num_channels x (board_x-2) x (board_y-2)\n",
        "        s = F.relu(self.bn4(self.conv4(s)))                          # batch_size x num_channels x (board_x-4) x (board_y-4)\n",
        "        s = s.view(-1, self.num_channels*(8-4)*(8-4))\n",
        "\n",
        "        s = F.dropout(F.relu(self.fc_bn1(self.fc1(s))), p=self.dropout, training=self.training)  # batch_size x 1024\n",
        "        s = F.dropout(F.relu(self.fc_bn2(self.fc2(s))), p=self.dropout, training=self.training)  # batch_size x 512\n",
        "\n",
        "        pi = self.fc3(s)                                                                         # batch_size x action_size\n",
        "        v = self.fc4(s)                                                                          # batch_size x 1\n",
        "\n",
        "        return F.softmax(pi, dim=1), torch.tanh(v)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-rtU84tNnvTq"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "db0Lcp8ZtopS"
      },
      "source": [
        "\n",
        "Where is the best place to gather the data? we could either store the positions in the play loop, or have the mcts return a list of every position that led to the end. What does each datapoint look like? Should be encoded board as input, then encoded action and p value as output\n",
        "But the p value should actually be the policy generated by the mcts, no? actually it looks like the p value isnt needed at all... why? \n",
        "Turns out I misunderstood. The outputs of the network are pi and v, where pi is the policy and v is the estimated value of the state. So we use the games outcome as the actual state value, and the q values as the policy output\n",
        "\n",
        "Tomorrow, should get to building the neural network and figuring out all the loss function stuff.\n",
        "Today I figured out how to collect the training points from the network\n",
        "TODO:\n",
        "- loss function\n",
        "- train loop\n",
        "- build real nn\n",
        "- collect data in self play loop\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf_66AibvsO-"
      },
      "source": [
        "# Basic PyTorch Setup\n",
        "### Dataset\n",
        "We will need to place our data into a DataLoader object so it can be iterated over in the training loop. This object will take our data set and deliver it to us in whatever batch size we ask for. We can also take our training data and place it into a Dataset object, which will be put into the dataloader.  \n",
        "\n",
        "### Model\n",
        "To make the model, we have it inherit from nn.Module, define the layers in the init function, and then show how the data passes through the layers in the forward function. We also have to make sure to move the model to the GPU if its available.\n",
        "\n",
        "### Optimize\n",
        "Create a loss function and an optimization function (usually SGD or Adam).\n",
        "\n",
        "### Train\n",
        "Train loop calls model.train() function (what is this?) takes the data from the dataloader and moves it to the GPU. It does a forward pass on the train data to get the predictions, then will measure the loss. Zero out the gradients on the optimizer and then compute the gradients from the loss. (Why do you have to zero out?). Then take a step with the optimizer to change the weights with the new gradients. You should also print out your training metrics like loss and epoch number periodically.\n",
        "\n",
        "### Test\n",
        "Load up some test data and then perform forward passes on all of it to get all the models predictions. You can turn off gradients to increase performance. Your test loss is the accumulation of all the loss functions from the test set, and your accuracy is the # of predictions that the model got correct.\n",
        "\n",
        "### Train/test loop\n",
        "One epoch in training would consist of training the model on the entire dataset and then testing the model. Its also common for people to save checkpoints of their model throughout training, just in case they had a model that performed pretty well in the past before it started to overfit."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hfp0O7iFyh3N"
      },
      "source": [
        "# Tensors\n",
        "Tensors work pretty much the name as numpy ndarrays, and can use the same underlying memory. The difference though is that tensors can run on a GPU and ndarrays only run on the CPU. I think most of the time id do this would be if I whipped up a numpy array to build some data (like in mcts) and then need to move it to the GPU for training.\n",
        "A tensor has a shape, dtype, and a device it exists on. Tensors are created on the CPU by default and have to be moved to the GPU (could you put them on the GPU directly so you dont have to move them?)  \n",
        "Dot Product: tensor.matmul(tensor.T) == tensor @ tensor.T  \n",
        "Element-wise product: tensor * tensor == tensor.mul(tensor)\n",
        "\n",
        "You can convert one element tensors, like sums, into a python numerical value using tensor.item(). Might be useful for loss functions.  \n",
        "\n",
        "# Autograd\n",
        "In order to get the gradients, you will need to set requires_grad=True on your tensors and then call for a backwards pass on your loss function (or any output you get that can give a gradient of your weights). Sometimes though you might want to disable gradient tracking in a network, like if you want to freeze some params when finetuning a network or to speed up computations when doing a forward pass. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezkdVzGXOUoW"
      },
      "outputs": [],
      "source": [
        "def load_batch(dataset, batch_size, random_sample=True):\n",
        "    if random_sample:\n",
        "        data_batch = random.sample(dataset, batch_size)\n",
        "    else:\n",
        "        data_batch = dataset[:batch_size]\n",
        "    x_list = []\n",
        "    pi_list = []\n",
        "    z_list = []\n",
        "\n",
        "    for datapoint in data_batch:\n",
        "        # Convert all datapoints into a training dataset if they pass the test\n",
        "        x, (pi, z) = datapoint.get_tensor()\n",
        "        x_list.append(x)\n",
        "        pi_list.append(pi)\n",
        "        z_list.append(z)\n",
        "      \n",
        "    return torch.stack(x_list), (torch.stack(pi_list), torch.stack(z_list).view(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlJOdcmyNeUb"
      },
      "outputs": [],
      "source": [
        "def train_loop(train_data, model, loss_fn, optimizer, device, batch_size=64):\n",
        "    size = len(train_data)\n",
        "    num_batches = size // batch_size\n",
        "    for i in range(0, num_batches):\n",
        "        X, (pi, z) = load_batch(train_data, batch_size)\n",
        "        X = X.to(device)\n",
        "        pi = pi.to(device)\n",
        "        z = z.to(device)\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        p, v = model(X)\n",
        "        assert p.shape == pi.shape\n",
        "        assert v.shape == z.shape\n",
        "        loss = loss_fn(p, v, pi, z)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            loss, current = loss.item(), i * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(test_data, model, loss_fn, device, batch_size=64):\n",
        "    size = len(test_data)\n",
        "    num_batches = size // batch_size\n",
        "    if not num_batches:\n",
        "        num_batches = 1\n",
        "        batch_size = size\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, num_batches):\n",
        "            X, (pi, z) = load_batch(test_data, batch_size)\n",
        "            X = X.to(device)\n",
        "            pi = pi.to(device)\n",
        "            z = z.to(device)\n",
        "\n",
        "            p, v = model(X)\n",
        "            assert p.shape == pi.shape\n",
        "            assert v.shape == z.shape\n",
        "            test_loss += loss_fn(p, v, pi, z).item()\n",
        "            correct += (p.argmax(1) == pi.argmax(1)).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXdJ22Zyv42Q"
      },
      "outputs": [],
      "source": [
        "def generate_games(net, iters=32, episodes=100):\n",
        "    # Generate train/test data\n",
        "    net.eval()\n",
        "    generated_data = generate_train_data(net, episodes=episodes, iters=iters)\n",
        "    random.shuffle(generated_data)\n",
        "    split_idx = 4*(len(generated_data) // 5)\n",
        "    train_data, test_data = generated_data[:split_idx], generated_data[split_idx:]\n",
        "    return train_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NR690h0r2lM"
      },
      "outputs": [],
      "source": [
        "def train_test_loop(net, train_data, test_data, epochs=50):\n",
        "    net.train()\n",
        "    for t in range(epochs):\n",
        "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "        train_loop(train_data, net, loss_fn, optimizer, device, batch_size=batch_size)\n",
        "        test_loop(test_data, net, loss_fn, device, batch_size=batch_size)\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0Q83Vlzoqkx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def save_checkpoint(nnet, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
        "    filepath = os.path.join(folder, filename)\n",
        "    if not os.path.exists(folder):\n",
        "        print(\"Checkpoint Directory does not exist! Making directory {}\".format(folder))\n",
        "        os.mkdir(folder)\n",
        "    else:\n",
        "        print(\"Checkpoint Directory exists! \")\n",
        "    torch.save({\n",
        "        'state_dict': nnet.state_dict(),\n",
        "    }, filepath)\n",
        "\n",
        "def load_checkpoint(nnet, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
        "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n",
        "    filepath = os.path.join(folder, filename)\n",
        "    if not os.path.exists(filepath):\n",
        "        raise (\"No model in path {}\".format(filepath))\n",
        "    map_location = None\n",
        "    checkpoint = torch.load(filepath, map_location=map_location)\n",
        "    nnet.load_state_dict(checkpoint['state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UT0vxu7hN3R",
        "outputId": "d29d9b5e-c05b-464f-82cf-303cc5af9fe4"
      },
      "outputs": [],
      "source": [
        "learning_rate = 5e-3\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B6Tk2LtvqKI"
      },
      "outputs": [],
      "source": [
        "# Create neural net\n",
        "net = ChessNet().to(device)\n",
        "loss_fn = loss\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-40MLrjpHk9"
      },
      "outputs": [],
      "source": [
        "# load_checkpoint(net, folder=\"drive/MyDrive/weights\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cztcp2vFsF91",
        "outputId": "d2cd2a88-cd53-4573-d63d-ae30c8a16a84"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    train_data, test_data = generate_games(net, iters=256, episodes=50)\n",
        "    train_test_loop(net, train_data, test_data, epochs=100)\n",
        "    save_checkpoint(net, folder=\"drive/MyDrive/weights\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dasulj3_2P4W"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "\n",
        "def test_net(net):\n",
        "    board = chess.Board()\n",
        "    tree = {'data': np.array((0,0,0,1)), 'color': board.turn}\n",
        "    while board.outcome() is None:\n",
        "        # Change exploration depending on the move count\n",
        "        if board.fullmove_number < 30:\n",
        "            temp = 1\n",
        "        else:\n",
        "            temp = 0\n",
        "        move, tree, pi = mcts_step(tree, net, board, temp=temp, iterations=256, epsilon=0.0)\n",
        "        board.push_uci(move)\n",
        "        display(board)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCeU3oYrIZDq"
      },
      "outputs": [],
      "source": [
        "# Profiling\n",
        "import cProfile, pstats, io\n",
        "\n",
        "def profile(fnc):\n",
        "    \n",
        "    \"\"\"A decorator that uses cProfile to profile a function\"\"\"\n",
        "    \n",
        "    def inner(*args, **kwargs):\n",
        "        \n",
        "        pr = cProfile.Profile()\n",
        "        pr.enable()\n",
        "        retval = fnc(*args, **kwargs)\n",
        "        pr.disable()\n",
        "        s = io.StringIO()\n",
        "        sortby = 'cumulative'\n",
        "        ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
        "        ps.print_stats()\n",
        "        print(s.getvalue())\n",
        "        return retval\n",
        "\n",
        "    return inner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9_I2RUOriav"
      },
      "outputs": [],
      "source": [
        "# Train and play loop\n",
        "net.eval()\n",
        "test_net(net)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
